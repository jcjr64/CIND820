When is a Liability not a Liability? Textual Analysis, Dictionaries, and 10-Ks
You can visit their website here to get more information about the dictionary. For our project, we just need to know the structure of the dictionary. The dictionary is in xlsx format, where we have all possible words that can occur in financial documents and different sentiments.

sentiment_categories = ['Negative', 'Positive', 'Uncertainty', 'Litigious', 
                             'Strong_Modal', 'Weak_Modal', 'Constraining']
Corresponding to each sentiment category, we have the year when it was added to that sentiment. Suppose a column has a value of 2009 for a positive column, then it means that the words were added as a positive sentiment in 2009. If it is -2009, then it was removed as a positive word in 2009. This helps us in avoiding survivorship bias and look-ahead bias.
So this tutorial is divided into three segments (i)Downloading data from SEC downloader, (ii)Text extract using regular expressions (iii)Sentiment analysis using Loughran-McDonald Master Dictionary
DOWNLOADING DATA FROM SEC-DOWNLOADER
You can refer to the repo here
from sec_edgar_downloader import Downloader

dl = Downloader("")
#Get the latest 2 10-Q documents including the amends
dl.get("10-Q","TSLA",include_amends=True,amount=2)
This will save in the folder sec-edgar-filings in an HTML format
TEXT EXTRACTION USING REGULAR EXPRESSIONS
First, we need to read the HTML file by using this function
def read_data(file_path:str):
    '''
    file_path: str Path to the HTML file
    Returns the text for the entire SEC filing
    '''
    HTMLFile = open(file_path, "r")
  
    index = HTMLFile.read()
  
    S = BeautifulSoup(index, 'lxml')

    return S.get_text(separator=' ',strip='\n')
Now, for the extraction of data, we have to use regex

def to_search(match_elem):
    list_match = match_elem.split(" ")
    list_match[0] = list_match[0].capitalize()
    return ' '.join(list_match)

def get_section_text(section:str,filing_text):
    # Remove these characters and replace them with empty string
    filing_text = re.sub('\xa0','',filing_text)
    filing_text = re.sub('\n','',filing_text)
    filing_text = re.sub('\t','',filing_text)
    if section[-1]!='.':
        section+='.'
    
    #Match all the ITEM elements in the document
    matches = list(re.finditer(re.compile('(Item|ITEM)\s?\d{0,2}[A-Z]?\.'),filing_text))
    # for match in matches:print(match)
    sections_list = [m[0] for m in matches]
    print(len(sections_list),sections_list)
    assert section in sections_list, f"The section should be in the list {sections_list}"
    item_matches = [i for i in range(len(matches)) if matches[i][0] == section]
    print("Item matches:",item_matches)
    
    section_name = ''
    if len(item_matches)>1: 
        section_name_start = min(item_matches)
        section_name_end = section_name_start+1
        start = matches[section_name_start].span()[1]
        end = matches[section_name_end].span()[0]
    
        section_name = filing_text[start:end]
        section_name = re.sub('\d','',section_name)
        
    start = max(item_matches)

    end = start+1
    start = matches[start].span()[1]
    end = matches[end].span()[0]

    section_text = filing_text[start:end]
    section_text = re.sub(section_name,'',section_text,count=1)
    section_text = re.sub(section_name.upper(),'',section_text,count=1)

    return section_text,section_name.strip()
The SEC filings look like the following

There are multiple sections that talk about the current financials of the company. It is a bit different for 10-K and 10-Q, but they have the same heading of Item. So the regex is trying to search all these texts under these headings to do sentiment analysis.
You can pass the argument for the Item that you want to fetch the data, and the regex will generate those data.
SENTIMENT ANALYSIS USING THE DICTIONARY
Preprocessing: We will be removing the stop words and as per the documentation we will lemmatize all the words.
import nltk
from nltk.stem.porter import *
nltk.download('punkt')
from nltk.tokenize import word_tokenize 
from nltk.stem import WordNetLemmatizer
import nltk
nltk.download('wordnet')
from tqdm import tqdm

stopwords = ['ME', 'MY', 'MYSELF', 'WE', 'OUR', 'OURS', 'OURSELVES', 'YOU', 'YOUR', 'YOURS',
                  'YOURSELF', 'YOURSELVES', 'HE', 'HIM', 'HIS', 'HIMSELF', 'SHE', 'HER', 'HERS', 'HERSELF',
                  'IT', 'ITS', 'ITSELF', 'THEY', 'THEM', 'THEIR', 'THEIRS', 'THEMSELVES', 'WHAT', 'WHICH',
                  'WHO', 'WHOM', 'THIS', 'THAT', 'THESE', 'THOSE', 'AM', 'IS', 'ARE', 'WAS', 'WERE', 'BE',
                  'BEEN', 'BEING', 'HAVE', 'HAS', 'HAD', 'HAVING', 'DO', 'DOES', 'DID', 'DOING', 'AN',
                  'THE', 'AND', 'BUT', 'IF', 'OR', 'BECAUSE', 'AS', 'UNTIL', 'WHILE', 'OF', 'AT', 'BY',
                  'FOR', 'WITH', 'ABOUT', 'BETWEEN', 'INTO', 'THROUGH', 'DURING', 'BEFORE',
                  'AFTER', 'ABOVE', 'BELOW', 'TO', 'FROM', 'UP', 'DOWN', 'IN', 'OUT', 'ON', 'OFF', 'OVER',
                  'UNDER', 'AGAIN', 'FURTHER', 'THEN', 'ONCE', 'HERE', 'THERE', 'WHEN', 'WHERE', 'WHY',
                  'HOW', 'ALL', 'ANY', 'BOTH', 'EACH', 'FEW', 'MORE', 'MOST', 'OTHER', 'SOME', 'SUCH',
                  'NO', 'NOR', 'NOT', 'ONLY', 'OWN', 'SAME', 'SO', 'THAN', 'TOO', 'VERY', 'CAN',
                  'JUST', 'SHOULD', 'NOW', 'AMONG']

def preprocess_data(section_text):
    word_tokens = word_tokenize(section_text) 
    filtered_sentence = [] 
    
    for w in word_tokens: 
        if w not in stopwords: 
            filtered_sentence.append(w) 
    #Lemmatization
    lemmatizer = WordNetLemmatizer()
    filtered_sentence = [lemmatizer.lemmatize(plural) for plural in filtered_sentence]
    return filtered_sentence
Now, finally, we have to get the sentiment for all the words, where we iterate over all the words and increment the sentiment counts for each category.
def get_sentiment(filtered_sentence):
    sentiment_categories = ['Negative', 'Positive', 'Uncertainty', 'Litigious', 
                             'Strong_Modal', 'Weak_Modal', 'Constraining']

    all_words = list(sent_df['Word'])
    sentiments_dict = {}
    sentiments_dict = {key:0 for key in sentiment_categories}
    for word in tqdm(filtered_sentence):
        capital_word = word.upper()
        if capital_word in all_words:
            # print(capital_word)
            sub_df = sent_df.loc[sent_df['Word']==capital_word]
            for sent in sentiment_categories:
                if sub_df[sent].values[0]>0:sentiments_dict[sent]+=1 
    return sentiments_dict
This will generate the following sentiments
{'Negative': 55,
 'Positive': 61,
 'Uncertainty': 76,
 'Litigious': 49,
 'Strong_Modal': 20,
 'Weak_Modal': 36,
 'Constraining': 44}
